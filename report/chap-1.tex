\chapter{Background in optimal control problems\\}
\label{cha:OCP}

This chapter gives some background information of the theory behind optimal control (OCP). After a global introduction and the discussion about the time discretization and shooting option, the chapter is giving an introduction into model predictive control (MPC).\\

\subsection{Optimal control problem (OCP)}
"An optimal control problem determines the desired inputs and corresponding state trajectories to change the system from an initial state to a desired final state in an optimal way while satisfying some input and state constraints." \cite{Mercy2018}.

\begin{equation}
\label{eq:OCP}
\begin{aligned}
\min_{\bm{q},\bm{u}} \quad & \int_{0}^{T}l(\bm{q}(t),\bm{u}(t))dt + E(\bm{q}(T)) \\
\textrm{s.t.} \quad & \bm{\dot{q}}(t) = \bm{f}(\bm{q}(t), \bm{u}(t))\\
& \bm{q}(0)= \bm{q}_{0},\hspace{2 mm}\bm{q}(T)= \bm{q}_{T}    \\
& \bm{q}(t)\in Q,\hspace{3 mm} \bm{u}(t)\in U, \hspace{3 mm} t\in [0, T]
\end{aligned}
\end{equation}

$\bm{q}$ is called the state vector and contains all the states of the system. $\bm{u}$ is the control vector containing the controls. In theory the amount of states of an system can be infinite, but in order to sufficiently model a system only a good chosen set of states is needed to sufficiently describe the system. In vehicle control in order to describe the driving behaviour often kinematic variables like positions and velocities of the centre of gravity of the vehicle are chosen together with the yaw angle. This fully describes the position of a vehicle in a 2D plane. Typical controls $\bm{u}$ are steerwheelangle and the amount of throttle which can be directly linked the amount of propulsion force. Also higher order controls are possible.\\
The objective function $l$ of the optimization problem \ref{eq:OCP} is integrating over the desired control horizon $T$. In the objective function it is indicated what should be minimized and it is in function of the different states and controls. $E(\bm{q}(T))$ describes the terminal cost and the value of the integrated objective is reduced when the system comes closer to a predefined final state at the end of the control horizon.
The dynamics of the system are modelled by an explicit ordinary differential equation $\bm{\dot{q}}(t) = \bm{f}(\bm{q}(t), \bm{u}(t))$ and the evolution of the vehicle states during the control horizon are retrieved by integration of this ODE.
$Q$ and $U$ represent the constraints that can be put on respectively the optimization variables. It is worth noting that the control horizon time $T$ by itself can also be an optimization variable. What comes out of an OCP indicates what states will be visited by the system and which controls have to be applied in order to optimize the associated objective function with respect to predefined constraints. \cite{Panos_opti}\\ 

There is a difference between soft and hard constraints. A hard constraint directly demarcates the feasible solution areas $\bm{q}(t)\in Q,\hspace{3 mm} \bm{u}(t)\in U$. A soft constraint is added in the objective function $l$ and will contribute to a more optimal solution when it is better fulfilled. Also when a soft constraint is not met this can be an optimal solution which is not the case for a hard constraint. \cite{Yankov} \\

\subsection{Time discretization}\label{s:time_dis}

The optimal control problem \ref{eq:OCP} is continuous in time. This means that it has infinite dimensions, but to be able to run the optimization problem on digital systems there is need for discretization. There are several ways to do this which are summarized in Figure \ref{fig:discretization_m}.\cite{Gillis2019}
\begin{figure}[htp]
	\centering
	\includegraphics[width=0.8\textwidth]{discretized.eps}
	\caption{Overview of different discretization methods.}
	\label{fig:discretization_m}
\end{figure}

"Since direct methods are best suited to solve practically relevant OCPs" \cite{Mercy2018}, this thesis is following the direct method. To implement the discretization of time when using the Direct method, a time shooting approach is used.

\subsubsection{Time shooting}
A shooting approach makes use of a time grid. Time will be sampled and on every time instant the optimal control problem is assessed. On these discrete points constraints will not violated but there are no limits on the amount of violation between the different time samples. In order to reduce the amount of constraint violation the sampling rate should be taken high enough, while bearing in mind the extra optimization variables introduced and therefore calculation load. An other approach to take constraint violation between different sample points into account, is using spline based optimization formulations. \cite{Mercy2018}\\

Two different shooting approaches exist:\\
\begin{enumerate}
	\item Multiple shooting (MS)\\
	During multiple shooting every new time sample $ns\in \mathbb{N}$ new states and controls are introduced and taken as optimization variables.
	Input changes are only allowed on the different time discrete time instances which leads to a piece wise control input signal. The blue bars indicate in Figure \ref{fig:TS} (left) indicate the control value $U_i$ that is applied to the system.  The red dots are system states that are defined as optimization variables and are not constant during a time interval $\Delta T$. In order to make the connection between states at time $t_i$ and $t_{i+1}$, discrete time integration is used according to $\bm{q}(k+1) = \bm{f}(\bm{q}(k), \bm{u}(k))$. These connections are put as constraints in the discretized optimization formulation of \ref{eq:OCP} and are called in literature 'path closing constraints' \cite{Gillis2019}. \\ Equation \ref{eq:OCP_dis} shows \ref{eq:OCP} when discretized with the multiple shooting approach. 

	\begin{equation}
	\label{eq:OCP_dis}
	\begin{aligned}
	\min_{\bm{q}(.),\bm{u}(.)} \quad & \sum_{k = 0}^{N-1}l_{k}(\bm{q}_{k},\bm{u}_{k}) + E(\bm{q}_{N}) \\
	\textrm{s.t.} \quad & \bm{q}_{k+1} = \bm{f}(\bm{q}_{k}, \bm{u}_{k}) & k = [0,\cdots, N-1]\\
	& \bm{q}_{0}= \bm{q}_{measured} \\
	& \bm{q}_{k}\in Q,\hspace{3 mm} & k = [0,\cdots, N]\\
	& \bm{u}_{k}\in U \hspace{3 mm} & k = [0,\cdots, N-1]\\
	& \bm{q}_{N}\in Q_f,\hspace{3 mm} N \in \mathbb{N}
	\end{aligned}
	\end{equation}
	
	\item Single shooting (SS)\\  
	In the single shooting or sequential approach which is visualized in Figure \ref{fig:TS}(right), only the first state and the controls are taken as optimization variables. Other states during the time horizon are derived from the initial state and the applied control. This is achieved by substituting the states during the control horizon by the integration results from the initial state and the applied controls. The mathematical formulation of this is shown by \ref{eq:2}. In Figure \ref{fig:TS} (right) are the states that result from integration indicated in green. \cite{Gillis2019}
	
	\begin{equation}\label{eq:2}
	\begin{aligned}
	\bm{q}^1 &= \bm{f}(\bm{q}^0, \bm{u}^0)\\
	\bm{q}^2 &= \bm{f}(\bm{f}(\bm{q}^0, \bm{u}^0), \bm{u}^1)\\
	\bm{q}^3 &= \bm{f}(\bm{f}(\bm{f}(\bm{q}^0, \bm{u}^0), \bm{u}^1), \bm{u}^2)\\
	...
	\end{aligned}
	\end{equation}
\end{enumerate}
%\vspace{1 cm}

\begin{figure}[htp]
	\centering
	\begin{minipage}{0.49\textwidth}
		\centering
		\includegraphics[width=1\textwidth]{MS_int.eps}
	\end{minipage}
	\hfill
	\begin{minipage}{.49\textwidth}
		\centering
		\includegraphics[width=1\textwidth]{SS.eps}
	\end{minipage}
	\caption{Schematic view of the time shooting approaches (left: multiple shooting; right: single shooting).}
	\label{fig:TS}
\end{figure}

In this thesis a multiple shooting approach is used together with the use of a Runge-Kutta integration scheme. Runge-Kutta is an explicit integration scheme which has a higher calculation cost than a standard Euler scheme but is more reliable for non-linear systems and has a higher stability with respect to the chosen time-step. \cite{Mercy2018}  \\ 

The difference between the two shooting approaches is that 'Multiple shooting'(MS) will lead to a larger Hessian of the objective and a larger Jacobian of the constraints. The reason for this is the introduction of more optimization variables. The advantage of MS is then again that these matrices are more sparse because a certain state is only dependent on the previous one and a control input which means they can be used in calculations more efficiently. In single shooting every state depends on the begin state and different controls which gives smaller, more populated matrices which are more calculation expensive. \cite{Gillis2019}

\subsection{Model predictive control}
In slow changing environments as for example a chemical plant, MPC is already a mature approach. More recently this technology also made its introduction in controlling systems with higher dynamics e.g. vehicle control due to an increase of computational power and the use of more efficiently algorithms. \cite{Mercy2018}. In the next section a short introduction of the formulation is made. \\

MPC is an approach were optimizations are solved in a loop in order to be able to notice disturbances, changing environments and model-plant mismatches. Therefore it makes use of a moving time horizon\footnote{There are also other implementations e.g. shrinking time horizon, where the prediction time gets smaller every step.}. Every iteration an OCP is solved over the prediction horizon and as result control signals are given as output. Only the first control is applied for one time interval of the finite prediction horizon $N\cdot T_{s}$ as is visualized in Figures \ref{fig:MPC1}and \ref{fig:MPC2}.

The decision on the amount of samples $N$ that are used in the control horizon is based on a trade off between higher accuracy and calculation effort. \cite{TongDuySon2019, Mercy2018}. In Figure \ref{fig:MPC1} the solving of an OCP during one MPC iteration on time sample $t+1$ is depicted. Figure \ref{fig:MPC2} shows than only the first control of the obtained control signal will be applied, which induces the system to come in a different state which will be the new start state and a new OCP will be solved. This will go on until the final desired conditions are met. A single OCP iteration only takes the model of the system into account as is done in a feedforward controller but over the iterations a feedback loop is acquired because of the changing initial state \cite{Patrinos2019}\\

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.6\textwidth]{MPC1.PNG}
	\caption{Visualization of the optimal control problem solved in one iteration of the MPC (Source: \cite{Patrinos2019}).}
	\label{fig:MPC1}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.6\textwidth]{MPC2.PNG}
	\caption{Visualization of the application of the first step of the calculated control signal during one iteration of the MPC (Source:\cite{Patrinos2019}).}
	\label{fig:MPC2}
\end{figure}

\newpage





%\begin{equation}\label{eq:3}
%minimize_{\bm{q}(.),\bm{u}(.)}\sum_{k = 0}^{N-1}l_{k}(\bm{q}_{k},\bm{u}_{k}) + E(\bm{q}_{N})
%\end{equation}
%\hspace{42 mm} \textit{subject to:}
%\[\bm{\dot{q}}_{k+1} = \sum_{k = 0}^{N-1}\bm{f}(\bm{q}_{k}, \bm{u}_{k})\]
%\[\bm{q}_{0}= \bm{q}_{measured}\]
%\[h(\bm{q}_{k},\bm{u}_{k}) \geq 0\]
%\[\bm{q}_{k}\in Q,\hspace{3 mm} \bm{u}_{k}\in U, \hspace{3 mm} N \in \mathbb{N}\]


Equation \ref{eq:MPC} is a representation of the solved discrete system OCP during one iteration of the MPC. It is a discretized version of equation \ref{eq:OCP}. The Runge-Kutta integration is embedded in $\bm{f}$. The hard constraints are represented by $h$. It is worth noticing that the constraints can be violated in-between the different time sample points.\\ \cite{Panos_opti}

MPC has no direct feedback loop, but through the iterative way of solving the OCPs it can still deal with model mismatch or a changing environment. The downside of this approach is that it requires a bigger computational load, which makes efficiently written software a necessity. 


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 